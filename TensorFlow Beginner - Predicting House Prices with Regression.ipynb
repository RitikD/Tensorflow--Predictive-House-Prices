{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Introduction\n",
    "\n",
    "---\n",
    "\n",
    "For this project, we are going to work on evaluating price of houses given the following features:\n",
    "\n",
    "1. Year of sale of the house\n",
    "2. The age of the house at the time of sale\n",
    "3. Distance from city center\n",
    "4. Number of stores in the locality\n",
    "5. The latitude\n",
    "6. The longitude\n",
    "\n",
    "![Regression](images/regression.png)\n",
    "\n",
    "Note: This notebook uses `python 3` and these packages: `tensorflow`, `pandas`, `matplotlib`, `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Importing Libraries & Helper Functions\n",
    "\n",
    "First of all, we will need to import some libraries and helper functions. This includes TensorFlow and some utility functions that I've written to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 3.8 kB/s eta 0:00:01    |██▏                             | 34.3 MB 3.3 MB/s eta 0:02:27     |███▎                            | 53.2 MB 1.7 MB/s eta 0:04:30     |████                            | 64.6 MB 6.4 MB/s eta 0:01:11     |█████                           | 82.2 MB 220 kB/s eta 0:32:53     |███████▎                        | 118.0 MB 4.0 MB/s eta 0:01:41     |████████                        | 128.1 MB 1.5 MB/s eta 0:04:13     |█████████████████▌              | 282.3 MB 5.2 MB/s eta 0:00:45     |████████████████████            | 321.5 MB 7.2 MB/s eta 0:00:28     |████████████████████▏           | 326.2 MB 4.0 MB/s eta 0:00:48     |██████████████████████████▍     | 424.8 MB 5.5 MB/s eta 0:00:17     |████████████████████████████▉   | 464.8 MB 693 kB/s eta 0:01:15\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/ritikd/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 78 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 70 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ritikd/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.34.2)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 283 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 12 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ritikd/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 176 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ritikd/anaconda3/lib/python3.6/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ritikd/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.4.1)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /home/ritikd/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.2)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ritikd/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ritikd/.local/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 842 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ritikd/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (45.2.0.post20200210)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 648 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /home/ritikd/.local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ritikd/.local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ritikd/.local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ritikd/.local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.9)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 42 kB/s  eta 0:00:011\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ritikd/anaconda3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.5.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 535 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ritikd/anaconda3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.2.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=08e437762bf9046b617e9b196c5ad7c5ebcb293a5321c5cb2d0d1218d4cb98a4\n",
      "  Stored in directory: /home/ritikd/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=6de4201b1ad8652b42d636afdaab628c40f550def1bf5c0c86436f79931ef077\n",
      "  Stored in directory: /home/ritikd/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "Successfully built termcolor absl-py\n",
      "Installing collected packages: absl-py, grpcio, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, tensorboard-plugin-wit, tensorboard, gast, keras-preprocessing, tensorflow-estimator, google-pasta, termcolor, opt-einsum, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.18.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 keras-preprocessing-1.1.2 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.12.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 3.2 kB/s eta 0:00:01    |██▋                             | 41.3 MB 5.3 MB/s eta 0:01:31     |██▋                             | 42.1 MB 5.3 MB/s eta 0:01:30     |███▉                            | 61.8 MB 5.3 MB/s eta 0:01:27     |████▍                           | 70.3 MB 175 kB/s eta 0:42:23     |████▌                           | 72.4 MB 964 kB/s eta 0:07:41     |████████▉                       | 142.0 MB 5.4 MB/s eta 0:01:10     |█████████▍                      | 151.4 MB 7.4 MB/s eta 0:00:50     |██████████                      | 162.4 MB 3.2 MB/s eta 0:01:50     |███████████▍                    | 183.8 MB 7.1 MB/s eta 0:00:47     |████████████████▌               | 265.9 MB 4.2 MB/s eta 0:01:00     |██████████████████▌             | 297.7 MB 9.5 MB/s eta 0:00:24     |███████████████████▊            | 317.4 MB 1.2 MB/s eta 0:02:40     |█████████████████████▎          | 343.5 MB 2.7 MB/s eta 0:01:05     |██████████████████████████▌     | 428.0 MB 2.3 MB/s eta 0:00:38     |██████████████████████████████▊ | 495.5 MB 2.2 MB/s eta 0:00:10     |██████████████████████████████▉ | 496.8 MB 2.2 MB/s eta 0:00:09\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Using cached protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 33 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Processing /home/ritikd/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679/absl_py-0.9.0-py3-none-any.whl\n",
      "Collecting numpy<2.0,>=1.16.0\n",
      "  Downloading numpy-1.19.0-cp36-cp36m-manylinux2010_x86_64.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 69 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 20 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Processing /home/ritikd/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-49.1.0-py3-none-any.whl (789 kB)\n",
      "\u001b[K     |████████████████████████████████| 789 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 12 kB/s  eta 0:00:011\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 764 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69740 sha256=699a039e2eaa26625eca333cbbfc0038fbe9147583d2dbb0a3cc8ad4726e2caa\n",
      "  Stored in directory: /home/ritikd/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built wrapt\n",
      "\u001b[31mERROR: spyder 4.0.1 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: spyder 4.0.1 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: astroid 2.3.3 has requirement wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wheel, six, astunparse, setuptools, protobuf, numpy, h5py, wrapt, gast, grpcio, opt-einsum, werkzeug, chardet, urllib3, idna, certifi, requests, oauthlib, requests-oauthlib, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, zipp, importlib-metadata, markdown, tensorboard-plugin-wit, absl-py, tensorboard, tensorflow-estimator, scipy, google-pasta, keras-preprocessing, termcolor, tensorflow-gpu\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 gast-0.3.3 google-auth-1.18.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 idna-2.10 importlib-metadata-1.7.0 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.19.0 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.12.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 setuptools-49.1.0 six-1.15.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0 tensorflow-estimator-2.2.0 tensorflow-gpu-2.2.0 termcolor-1.1.0 urllib3-1.25.9 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1 zipp-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --ignore-installed --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
    "\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Importing the Data\n",
    "\n",
    "The dataset is saved in a `data.csv` file. We will use `pandas` to take a look at some of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', names = column_names) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Check Missing Data\n",
    "\n",
    "It's a good practice to check if the data has any missing values. In real world data, this is quite common and must be taken care of before any data pre-processing or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Data Normalization\n",
    "\n",
    "We can make it easier for optimization algorithms to find minimas by normalizing the data before training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,1:]\n",
    "df_norm = (df - df.mean()) / df.std()\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Convert Label Value\n",
    "\n",
    "Because we are using normalized values for the labels, we will get the predictions back from a trained model in the same distribution. So, we need to convert the predicted values back to the original distribution if we want predicted prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = df['price'].mean()\n",
    "y_std = df['price'].std()\n",
    "\n",
    "def convert_label_value(pred):\n",
    "    return int(pred * y_std + y_mean)\n",
    "\n",
    "print(convert_label_value(0.350088))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Create Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Select Features\n",
    "\n",
    "Make sure to remove the column __price__ from the list of features as it is the label and should not be used as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_norm.iloc[:, :6]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: Select Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_norm.iloc[:, -1]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3: Feature and Label Values\n",
    "\n",
    "We will need to extract just the numeric values for the features and labels as the TensorFlow model will expect just numeric values as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arr = X.values\n",
    "Y_arr = Y.values\n",
    "\n",
    "print('X_arr shape: ', X_arr.shape)\n",
    "print('Y_arr shape: ', Y_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4: Train and Test Split\n",
    "\n",
    "We will keep some part of the data aside as a __test__ set. The model will not use this set during training and it will be used only for checking the performance of the model in trained and un-trained states. This way, we can make sure that we are going in the right direction with our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_arr, Y_arr, test_size = 0.05, shuffle = True, random_state=0)\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Create the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1: Create the Model\n",
    "\n",
    "Let's write a function that returns an untrained model of a certain architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(10, input_shape = (6,), activation = 'relu'),\n",
    "        Dense(20, activation = 'relu'),\n",
    "        Dense(5, activation = 'relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer='adadelta'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1: Model Training\n",
    "\n",
    "We can use an `EarlyStopping` callback from Keras to stop the model training if the validation loss stops decreasing for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 5)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "preds_on_untrained = model.predict(X_test)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs = 1000,\n",
    "    callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2: Plot Training and Validation Loss\n",
    "\n",
    "Let's use the `plot_loss` helper function to take a look training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1: Plot Raw Predictions\n",
    "\n",
    "Let's use the `compare_predictions` helper function to compare predictions from the model when it was untrained and when it was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_on_trained = model.predict(X_test)\n",
    "\n",
    "compare_predictions(preds_on_untrained, preds_on_trained, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2: Plot Price Predictions\n",
    "\n",
    "The plot for price predictions and raw predictions will look the same with just one difference: The x and y axis scale is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_on_untrained = [convert_label_value(y) for y in preds_on_untrained]\n",
    "price_on_trained = [convert_label_value(y) for y in preds_on_trained]\n",
    "price_y_test = [convert_label_value(y) for y in y_test]\n",
    "\n",
    "compare_predictions(price_on_untrained, price_on_trained, price_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
